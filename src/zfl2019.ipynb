{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Land Use Classification and Machine Learning\n",
    "## ZFL, Bonn March/April 2019\n",
    "\n",
    "### Outline\n",
    "Land cover classification of remote sensing imagery is a task which falls into the general category of pattern recognition. Pattern recognition problems, in turn, are usually approached by developing appropriate machine learning algorithms. Broadly speaking, machine learning involves tasks for which there is no known direct method to compute a desired output from a set of inputs. The strategy adopted is for the computer to learn from a set of representative examples.\n",
    "\n",
    "In the case of supervised classification, the task can often be seen as one of modeling probability distributions.  This is called the training phase of the classification procedure. Then these probabilities are used to classify all of the pixels in the image, a step  referred to as the generalization phase.\n",
    "\n",
    "The course will treat three representative models for supervised classification which involve probability estimation: a parametric model (the Bayes maximum likelihood classifier), a nonparametric model (Gaussian kernel classification), and a semiparametric or mixture model (the feed-forward neural network or FFN). In addition statistical methods for accuracy assessment and model comparison will be discussed.\n",
    "\n",
    "The theory will be illustrated with Python programs for classification and evaluation, both for local processing as well as on the Google Earth Engine. In the case of neural networks, deep learning techniques with TensorFlow will be introduced.\n",
    "\n",
    "\"Deep learning is a particular kind of machine learning that achieves great power and ﬂexibility by representing the world as a nested hierarchy of concepts, with each concept deﬁned in relation to simpler concepts, and more abstract representations computed in terms of less abstract ones.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# these are innocuous but irritating\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")\n",
    "# enable in-line graphics\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data separability\n",
    "$$\n",
    "\\epsilon = \\int\\min[\\ p(g\\mid 1)Pr(1),p(g\\mid 2)Pr(2)\\ ]dg.\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "B = {1\\over 8}(\\mu_2-\\mu_1)^\\top \\left[{\\Sigma_1+\\Sigma_2\\over 2}\\right]^{-1} (\\mu_2-\\mu_1)\n",
    "+{1\\over 2}\\log\\left({\\left|\\Sigma_1+\\Sigma_2\\right|/2\\over \\sqrt{|\\Sigma_1||\\Sigma_2|}}\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training regions are in ENVI shapefiles in the imagery directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls imagery | grep train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and these have been uploaded to GEE as a table in the shared Asset\n",
    "\n",
    "    users/mortcanty/supervisedclassification/train\n",
    "    \n",
    "together with the 9-band ASTER PCA image \n",
    "\n",
    "    users/mortcanty/supervisedclassification/AST_20070501_pca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "ee.Initialize()\n",
    "\n",
    "# first 4 principal components of ASTER image\n",
    "image = ee.Image('users/mortcanty/supervisedclassification/AST_20070501_pca') \\\n",
    "            .select(0,1,2,3)\n",
    "\n",
    "# training data\n",
    "table = ee.FeatureCollection('users/mortcanty/supervisedclassification/train')\n",
    "trainData = image.sampleRegions(table,['CLASS_ID'])\n",
    "print trainData.size().getInfo()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to calculate the Jeffries-Matusita separability on the GEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jmsep(class1,class2,image,table):\n",
    "# Jeffries-Matusita separability    \n",
    "    table1 = table.filter(\n",
    "        ee.Filter.eq('CLASS_ID',str(class1-1)))\n",
    "    m1 = image.reduceRegion(ee.Reducer.mean(),table1)\\\n",
    "              .toArray() \n",
    "    s1 = image.toArray() \\\n",
    "         .reduceRegion(ee.Reducer.covariance(),table1)\\\n",
    "         .toArray()\n",
    "    table2 = table.filter(\n",
    "        ee.Filter.eq('CLASS_ID',str(class2-1)))\n",
    "    m2 = image.reduceRegion(ee.Reducer.mean(),table2)\\\n",
    "              .toArray()\n",
    "    s2 = image.toArray() \\\n",
    "        .reduceRegion(ee.Reducer.covariance(),table2,15)\\\n",
    "              .toArray()\n",
    "    m12 = m1.subtract(m2)  \n",
    "    m12 = ee.Array([m12.toList()]) # makes 2D matrix  \n",
    "    s12i = s1.add(s2).divide(2).matrixInverse()\n",
    "#  first term in Bhattacharyya distance\n",
    "    B1 = m12.matrixMultiply(\n",
    "          s12i.matrixMultiply(m12.matrixTranspose())) \\\n",
    "            .divide(8)\n",
    "    ds1 = s1.matrixDeterminant()\n",
    "    ds2 = s2.matrixDeterminant() \n",
    "    ds12 = s1.add(s2).matrixDeterminant()\n",
    "#  second term\n",
    "    B2 = ds12.divide(2).divide(ds1.multiply(ds2).sqrt())\\\n",
    "             .log().divide(2)\n",
    "    B = ee.Number(B1.add(B2).project([0]).toList().get(0))\n",
    "#  J-M separability\n",
    "    return ee.Number(1).subtract(ee.Number(1) \\\n",
    "             .divide(B.exp())) \\\n",
    "             .multiply(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print jmsep(5,9,image,table).getInfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Band mean values for the training data, illustrating iteration over an ee.List object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def band_mean(current,prev):\n",
    "    current = ee.String(current)\n",
    "    prev = ee.Dictionary(prev)\n",
    "    trainData = ee.FeatureCollection(prev.get('trainData'))\n",
    "    class_id = prev.get('class_id')\n",
    "    means = ee.List(prev.get('means'))\n",
    "    mu = trainData.filter(ee.Filter.eq('CLASS_ID',class_id)).aggregate_mean(current)\n",
    "    return ee.Dictionary({ 'trainData':trainData,'class_id':class_id,'means':means.add(mu) })\n",
    "\n",
    "def class_mean(trainData,class_id,bandNames):\n",
    "    first = ee.Dictionary({'trainData':trainData,'class_id':str(class_id),'means':ee.List([])})\n",
    "    return ee.Dictionary(bandNames.iterate(band_mean,first)).get('means')\n",
    "\n",
    "mu = ee.Array(class_mean(trainData,9,image.bandNames()))\n",
    "print mu.getInfo()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Maximum Likeliood Classifier (parametric model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes on the GEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as disp\n",
    "jet = 'black,blue,cyan,yellow,red,brown'\n",
    "\n",
    "# rename the class ids from strings to integers\n",
    "trainData = trainData\\\n",
    "    .remap(['0','1','2','3','4','5','6','7','8','9'],\n",
    "           [0,1,2,3,4,5,6,7,8,9],'CLASS_ID')\n",
    "    \n",
    "# train a naive Bayes classifier    \n",
    "classifier = ee.Classifier.continuousNaiveBayes()\n",
    "trained = classifier\\\n",
    "    .train(trainData,'CLASS_ID',image.bandNames())\n",
    "\n",
    "# classify the image and display    \n",
    "classified = image.classify(trained)\n",
    "url = classified.select('classification')\\\n",
    "    .getThumbURL({'min':0,'max':9,'palette':jet})\n",
    "disp.Image(url=url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Bayes Maximum Likelihood with the classify.py script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run scripts/classify -p [1,2,3,4] -a 1 imagery/AST_20070501_pca.tif imagery/train.shp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run scripts/dispms -f imagery/AST_20070501_pca_class.tif -c \\\n",
    "-r  \"['WATER', 'RAPESEED', 'SUGARBEET', 'SUBURBAN', 'INDUSTRIAL', 'CONIFEROUS', 'GRAIN', 'GRASSLAND', 'HERBIFEROUS', 'OPENCAST']\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gauss Kernel Classifier (non-parametric model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run scripts/classify -p [1,2,3,4] -a 2 -P imagery/AST_20070501_pca.tif imagery/train.shp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward Neural Network (mixture model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation in low-level TensorFlow\n",
    "#### Network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "# placeholders\n",
    "Gs = tf.placeholder(tf.float32,shape=(None,4))\n",
    "ls = tf.placeholder(tf.int64,shape=(None))\n",
    "# hidden layer with rectified linear units (relu) \n",
    "hidden=tf.layers.dense(Gs,10,activation=tf.nn.relu)\n",
    "# output layer\n",
    "logits=tf.layers.dense(hidden,10)\n",
    "# cross entropy cost function\n",
    "xentropy=tf.nn.sparse_softmax_cross_entropy_with_logits\\\n",
    "                            (labels=ls,logits=logits)\n",
    "cost=tf.reduce_mean(xentropy)\n",
    "# training algorithm with 0.01 learning rate\n",
    "optimizer=tf.train.GradientDescentOptimizer(0.01)\n",
    "training_op=optimizer.minimize(cost)\n",
    "# variables initializer \n",
    "init=tf.global_variables_initializer()\n",
    "# accuracy evaluation\n",
    "correct=tf.nn.in_top_k(logits,ls,1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct,tf.float32))\n",
    "# saver\n",
    "saver = tf.train.Saver()\n",
    "# logger\n",
    "cost_summary = tf.summary.scalar('COST',cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting train/test data from GEE to Google Drive for classification with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample the image with the polygons to a feature  \n",
    "# collection, rename the class id columns from strings to \n",
    "# integers and add a column of random numbers in [0,1]\n",
    "trainTestData = trainData.randomColumn('rand',seed=12345) \n",
    "    \n",
    "# filter on the random column to split into training and test\n",
    "# feature collections in the ration of 2:1\n",
    "trainData = trainTestData.filter(ee.Filter.lt('rand',0.67))\n",
    "testData = trainTestData.filter(ee.Filter.gte('rand',0.67))\n",
    "\n",
    "print 'train pixels: %i'%trainData.size().getInfo()\n",
    "print 'test pixels:  %i'%testData.size().getInfo()    \n",
    "\n",
    "# Export feature collections as csv files\n",
    "gdexport = ee.batch.Export.table.toDrive(trainData,description='driveexporttask',folder= 'EarthEngineImages',fileNamePrefix='traindata')    \n",
    "gdexport.start() \n",
    "\n",
    "gdexport = ee.batch.Export.table.toDrive(testData,description='driveexporttask',folder= 'EarthEngineImages',fileNamePrefix='testdata')    \n",
    "gdexport.start()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< Download the CSV files from Google Drive to myimagery folder>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls myimagery | grep csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in as Pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dftrain = pd.read_csv('myimagery/traindata.csv')\n",
    "dftest = pd.read_csv('myimagery/testdata.csv')\n",
    "print dftrain.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert relevant columns to numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gstrn = dftrain.values[:,2:6]\n",
    "lstrn = dftrain.values[:,1]\n",
    "\n",
    "Gstst = dftest.values[:,2:6]\n",
    "lstst = dftest.values[:,1]\n",
    "\n",
    "print Gstrn.shape\n",
    "print lstrn.shape\n",
    "print Gstst.shape\n",
    "print lstst.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "logdir = 'tf_logs/run-'+str(now)\n",
    "file_writer = tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(5000):\n",
    "        if epoch % 200 ==0:\n",
    "            summary_str = cost_summary.eval(feed_dict={Gs:Gstrn,ls:lstrn})\n",
    "            file_writer.add_summary(summary_str,epoch)\n",
    "        sess.run(training_op,feed_dict={Gs:Gstrn,ls:lstrn})\n",
    "    acc = accuracy.eval(feed_dict={Gs:Gstst,ls:lstst})\n",
    "    file_writer.close()\n",
    "    print 'Test accuracy: %f'%acc\n",
    "    save_path = saver.save(sess,'imagery/dnn.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!tensorboard --logdir tf_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import auxil.readshp as rs\n",
    "from osgeo import gdal\n",
    "from osgeo.gdalconst import GA_ReadOnly\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "infile='imagery/AST_20070501_pca.tif'\n",
    "gdal.AllRegister()\n",
    "inDataset = gdal.Open(infile,GA_ReadOnly)\n",
    "# read entire image\n",
    "cols = inDataset.RasterXSize\n",
    "rows = inDataset.RasterYSize  \n",
    "Gs_all = np.zeros((cols*rows,4))\n",
    "for b in range(4):\n",
    "    band = inDataset.GetRasterBand(b+1)\n",
    "    Gs_all[:,b] = band.ReadAsArray(0,0,cols,rows)\\\n",
    "                          .astype(float).ravel()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,'imagery/dnn.ckpt')\n",
    "    Z = logits.eval(feed_dict={Gs:Gs_all})\n",
    "    cls = np.argmax(Z,1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.imshow(np.reshape(cls/10.0,(rows,cols)),cmap='jet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation in high-level TensorFlow (keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run scripts/classify -p [1,2,3,4] -a 6 -e 1000 -L [10,10,10] imagery/AST_20070501_pca.tif imagery/train.shp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
